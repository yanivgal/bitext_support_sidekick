# Bitext Support Sidekick ğŸ¤–

A Streamlit-based chat application that helps users analyze and understand the Bitext Customer Service Tagged Training dataset through natural language interactions. The dataset is automatically downloaded from HuggingFace on first run and cached locally.

## Overview

This project implements an intelligent agent that can answer questions about customer service data through a conversational interface. The agent can handle both structured queries (e.g., "What are the most frequent categories?") and unstructured analysis (e.g., "Summarize Category X").

## Features

- ğŸ¤– Interactive chat interface built with Streamlit
- ğŸ”„ Two agent modes:
  - **Reactive** â€“ step-by-step thinking and execution
  - **Plan** â€“ creates a structured plan before execution
- ğŸ“Š Data analysis capabilities:
  - Category analysis and distribution
  - Intent analysis
  - Semantic search
  - Exact search
  - Data aggregation
  - Common questions identification
- ğŸš¦ Automatic scope checking to filter out-of-topic questions
- ğŸ’¡ Transparent reasoning with expandable thinking steps
- ğŸ› ï¸ Tool-based architecture for modular functionality

## Architecture

The application follows a modular architecture:

- `app.py` â€“ Streamlit UI and chat flow
- `agent.py` â€“ orchestrates the conversation, scope checking and reasoning
- `brain/` â€“ planning and reactive strategies
- `chat/` â€“ message models and wrapper around the OpenAI API
- `bitext/datastore.py` â€“ loads the dataset and builds the search index
- `scope_checker/` â€“ verifies if a question is in scope
- `tools/` â€“ data analysis tools:
  - `data_slicer.py` â€“ filter/group/sort the data
  - `find_common_questions.py` â€“ discover frequent question patterns
  - `aggregator.py` â€“ aggregation functions
  - `exact_search.py` â€“ literal text search
  - `semantic_search.py` â€“ embedding based search
  - `dataset_info.py` â€“ dataset metadata
  - `calculator.py` â€“ numerical calculations

## Getting Started

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your API keys
```

3. Run the application:
```bash
streamlit run app.py
```

## Requirements

- Python 3.10 or higher is required for this project. The code relies on
  [PEP&nbsp;604](https://peps.python.org/pep-0604/) union types (e.g. `list | None`),
  which are not available in earlier Python versions.
- We recommend using a virtual environment to manage dependencies

## Usage

1. Select your preferred agent mode in the sidebar:
   - Reactive: For step-by-step thinking and execution
   - Plan: For structured planning before execution

2. Ask questions about the dataset in natural language:
   - "What are the most frequent categories?"
   - "Show examples of Category X"
   - "Summarize how agents respond to Intent Y"

3. View the agent's thinking process by expanding the "Thinking..." sections

## Project Structure

```
.
â”œâ”€â”€ app.py              # Streamlit entry point
â”œâ”€â”€ agent.py            # Conversation orchestration
â”œâ”€â”€ brain/              # Planning and reactive logic
â”œâ”€â”€ chat/               # Message models and OpenAI wrapper
â”œâ”€â”€ bitext/
â”‚   â””â”€â”€ datastore.py    # Dataset loader and embedding index
â”œâ”€â”€ scope_checker/      # Out-of-scope detection
â”œâ”€â”€ tools/              # Data analysis tools
â”‚   â”œâ”€â”€ data_slicer.py
â”‚   â”œâ”€â”€ find_common_questions.py
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â”œâ”€â”€ exact_search.py
â”‚   â”œâ”€â”€ semantic_search.py
â”‚   â”œâ”€â”€ dataset_info.py
â”‚   â””â”€â”€ calculator.py
â”œâ”€â”€ notebooks/          # Development notebooks
â””â”€â”€ requirements.txt    # Project dependencies
```

## Contributing

Feel free to submit issues and enhancement requests! 
